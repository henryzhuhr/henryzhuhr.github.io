<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <meta name="generator" content="VuePress 2.0.0-beta.49">
    <style>
      :root {
        --c-bg: #fff;
      }
      html.dark {
        --c-bg: #22272e;
      }
      html, body {
        background-color: var(--c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme');
			const systemDarkMode = window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
			if (userMode === 'dark' || (userMode !== 'light' && systemDarkMode)) {
				document.documentElement.classList.toggle('dark', true);
			}
    </script>
    <link rel="icon" type="image/png" sizes="32x32" href="/images/global/avatar-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/global/avatar-16x16.png"><meta name="application-name" content="Henry Zhu"><meta name="apple-mobile-web-app-title" content="Henry Zhu"><meta name="apple-mobile-web-app-status-bar-style" content="black"><link rel="apple-touch-icon" href="/images/global/avatar.png"><meta name="theme-color" content="#377bb5"><meta name="msapplication-TileColor" content="#377bb5"><title>论文 | Henry Zhu</title><meta name="description" content="">
    <link rel="modulepreload" href="/assets/app.e2a6bb87.js"><link rel="modulepreload" href="/assets/对抗纹理.html.8cc235f8.js"><link rel="modulepreload" href="/assets/对抗纹理.html.58078046.js"><link rel="prefetch" href="/assets/offer.html.df936bb7.js"><link rel="prefetch" href="/assets/resume.html.f7bb4442.js"><link rel="prefetch" href="/assets/deeplearning.html.912fadc7.js"><link rel="prefetch" href="/assets/about.html.1e2becfa.js"><link rel="prefetch" href="/assets/index.html.b6d87e98.js"><link rel="prefetch" href="/assets/cpu.html.1fe8dd9e.js"><link rel="prefetch" href="/assets/process.html.ceec3b98.js"><link rel="prefetch" href="/assets/mysql.html.4dcfd0d8.js"><link rel="prefetch" href="/assets/obsidian.html.f88c5bec.js"><link rel="prefetch" href="/assets/DiffusionModel.html.994a8cc3.js"><link rel="prefetch" href="/assets/index.html.64d46029.js"><link rel="prefetch" href="/assets/basic.html.4eae7658.js"><link rel="prefetch" href="/assets/classification.html.6af1adf1.js"><link rel="prefetch" href="/assets/classifier.html.2f5932c6.js"><link rel="prefetch" href="/assets/cnn.html.d721c89f.js"><link rel="prefetch" href="/assets/gan.html.0b7522e8.js"><link rel="prefetch" href="/assets/FaceAssign.html.547a928d.js"><link rel="prefetch" href="/assets/Initialization.html.f42fd297.js"><link rel="prefetch" href="/assets/MetricLearning.html.1ec6b598.js"><link rel="prefetch" href="/assets/ema.html.a3ade3eb.js"><link rel="prefetch" href="/assets/mlp.html.d35da666.js"><link rel="prefetch" href="/assets/nlp.html.b191e903.js"><link rel="prefetch" href="/assets/norm.html.52311e88.js"><link rel="prefetch" href="/assets/object_dection.html.6e439e2c.js"><link rel="prefetch" href="/assets/fine_tune.html.cb6d89e6.js"><link rel="prefetch" href="/assets/pytorch-install.html.c94377e6.js"><link rel="prefetch" href="/assets/train.html.e4124028.js"><link rel="prefetch" href="/assets/transformer.html.5383695d.js"><link rel="prefetch" href="/assets/index.html.2b9d0e8f.js"><link rel="prefetch" href="/assets/2019ICML_Zhang_TRADES.html.78ee293b.js"><link rel="prefetch" href="/assets/2019_Shafahi_LS.html.7388e530.js"><link rel="prefetch" href="/assets/2020ICML_Rice_RobustOverfitting.html.ba364817.js"><link rel="prefetch" href="/assets/2020NIPS_Pang_HE.html.472aa544.js"><link rel="prefetch" href="/assets/2021CVPR_Luo_Diffusion3DPointCloud.html.7938f0f8.js"><link rel="prefetch" href="/assets/2021ICML_radford_CLIP.html.70b8c04d.js"><link rel="prefetch" href="/assets/AdversarialTraining.html.4935c3b0.js"><link rel="prefetch" href="/assets/StyleGAN2.html.b1b68b00.js"><link rel="prefetch" href="/assets/latex.html.90635eb9.js"><link rel="prefetch" href="/assets/backtracking.html.6f341901.js"><link rel="prefetch" href="/assets/dp.html.f2f0487e.js"><link rel="prefetch" href="/assets/monotonic_stack.html.4d866b4f.js"><link rel="prefetch" href="/assets/wuhan23_sunset.html.d4c65045.js"><link rel="prefetch" href="/assets/多卡训练.html.56ebb72c.js"><link rel="prefetch" href="/assets/index.html.4addc43b.js"><link rel="prefetch" href="/assets/conda.html.ea4491e8.js"><link rel="prefetch" href="/assets/CXX.html.4c59db51.js"><link rel="prefetch" href="/assets/deploy.html.432f1f8e.js"><link rel="prefetch" href="/assets/mAP.html.16e8ac3f.js"><link rel="prefetch" href="/assets/design_pattern.html.a28a07d3.js"><link rel="prefetch" href="/assets/freertos.html.5c138d73.js"><link rel="prefetch" href="/assets/command.html.4dce3d49.js"><link rel="prefetch" href="/assets/git-zhihu.html.1707fc79.js"><link rel="prefetch" href="/assets/git.html.9b3ee30f.js"><link rel="prefetch" href="/assets/github.html.8c083bd9.js"><link rel="prefetch" href="/assets/http.html.b2667ce9.js"><link rel="prefetch" href="/assets/linux-ubuntu.html.b36c026a.js"><link rel="prefetch" href="/assets/linux.html.33aee037.js"><link rel="prefetch" href="/assets/macos.html.a0f8b6ca.js"><link rel="prefetch" href="/assets/mosquitto.html.7d3ab270.js"><link rel="prefetch" href="/assets/index.html.5e6b04ca.js"><link rel="prefetch" href="/assets/openvino.html.4fceed65.js"><link rel="prefetch" href="/assets/list.html.d6036b5f.js"><link rel="prefetch" href="/assets/python.html.9826fb66.js"><link rel="prefetch" href="/assets/OpenWRT.html.e26b2f68.js"><link rel="prefetch" href="/assets/homeAssistant-zhihu.html.177a50b1.js"><link rel="prefetch" href="/assets/homeAssistant.html.bd5d729c.js"><link rel="prefetch" href="/assets/icm20602.html.bd352ef8.js"><link rel="prefetch" href="/assets/raspberrypi.html.30d346d0.js"><link rel="prefetch" href="/assets/server.html.89127fdd.js"><link rel="prefetch" href="/assets/ros2.html.18a26329.js"><link rel="prefetch" href="/assets/rtmp.html.c8ea9db7.js"><link rel="prefetch" href="/assets/ssh.html.9dde9202.js"><link rel="prefetch" href="/assets/Vue-init.html.2eb3cf3e.js"><link rel="prefetch" href="/assets/index.html.cc33b72e.js"><link rel="prefetch" href="/assets/CodiMD.html.589db885.js"><link rel="prefetch" href="/assets/PromptEngineering.html.77c39590.js"><link rel="prefetch" href="/assets/QQ-GPT.html.8f31f0c8.js"><link rel="prefetch" href="/assets/utm串口挂载.html.cac09dc5.js"><link rel="prefetch" href="/assets/BlenderPython.html.071e0eb9.js"><link rel="prefetch" href="/assets/ContentAuthoring-vehicles.html.d5caadd9.js"><link rel="prefetch" href="/assets/apis.html.f9348ec0.js"><link rel="prefetch" href="/assets/carla-zhihu.html.582531f0.js"><link rel="prefetch" href="/assets/carla.html.6073e636.js"><link rel="prefetch" href="/assets/python-api.html.03da0082.js"><link rel="prefetch" href="/assets/retrieve_data.html.82afe08b.js"><link rel="prefetch" href="/assets/clash.html.0a63b049.js"><link rel="prefetch" href="/assets/pytorch3d.html.82de2770.js"><link rel="prefetch" href="/assets/vscode.html.62b5c605.js"><link rel="prefetch" href="/assets/start.html.5d82315f.js"><link rel="prefetch" href="/assets/dataset.html.dc2ad9f1.js"><link rel="prefetch" href="/assets/index.html.8bb260a7.js"><link rel="prefetch" href="/assets/diffusion_attack.html.ab4ca83e.js"><link rel="prefetch" href="/assets/cuda_cudnn.html.e9e14950.js"><link rel="prefetch" href="/assets/TensorRT.legacy.html.1bacb8fd.js"><link rel="prefetch" href="/assets/TensorRT.html.394e181c.js"><link rel="prefetch" href="/assets/cuda.html.b02c75b1.js"><link rel="prefetch" href="/assets/index.html.e696ffaf.js"><link rel="prefetch" href="/assets/JetsonXavierNX-zhihu.html.295219eb.js"><link rel="prefetch" href="/assets/jetson_xavier.html.ab49eb44.js"><link rel="prefetch" href="/assets/basic.html.5f2eba05.js"><link rel="prefetch" href="/assets/cmake.html.772f1c49.js"><link rel="prefetch" href="/assets/cxx11.html.f2a42b0f.js"><link rel="prefetch" href="/assets/hash_table.html.7c65b17a.js"><link rel="prefetch" href="/assets/tree.html.d33e294c.js"><link rel="prefetch" href="/assets/generics.html.764662d5.js"><link rel="prefetch" href="/assets/lambda.html.2940b2e0.js"><link rel="prefetch" href="/assets/rvalue.html.3cc1f080.js"><link rel="prefetch" href="/assets/safety_efficiency.html.0c07834f.js"><link rel="prefetch" href="/assets/smart_ptr.html.2b39e074.js"><link rel="prefetch" href="/assets/std_cincout.html.c626d036.js"><link rel="prefetch" href="/assets/stl.html.e78e9f0c.js"><link rel="prefetch" href="/assets/thread.html.46d79fc0.js"><link rel="prefetch" href="/assets/trick.html.d7e22552.js"><link rel="prefetch" href="/assets/java-env.html.53a572d4.js"><link rel="prefetch" href="/assets/basic.html.58840ade.js"><link rel="prefetch" href="/assets/linux-basic.html.2e1926d3.js"><link rel="prefetch" href="/assets/linux-file_system.html.edefa040.js"><link rel="prefetch" href="/assets/实验1 Linux.html.edabd78d.js"><link rel="prefetch" href="/assets/实验2 Makefile实验.html.68e8387d.js"><link rel="prefetch" href="/assets/cp.html.1bfc0291.js"><link rel="prefetch" href="/assets/frp.html.fee6dac9.js"><link rel="prefetch" href="/assets/tmux.html.da4d7637.js"><link rel="prefetch" href="/assets/ubuntu.html.d1c054a7.js"><link rel="prefetch" href="/assets/vim.html.6fe7ddf3.js"><link rel="prefetch" href="/assets/iterator.html.5db031c5.js"><link rel="prefetch" href="/assets/list.html.65f8b187.js"><link rel="prefetch" href="/assets/icm20602.html.3e087edd.js"><link rel="prefetch" href="/assets/404.html.7d858b3d.js"><link rel="prefetch" href="/assets/index.html.de26035a.js"><link rel="prefetch" href="/assets/index.html.4a704eac.js"><link rel="prefetch" href="/assets/index.html.aa248416.js"><link rel="prefetch" href="/assets/index.html.e17824b6.js"><link rel="prefetch" href="/assets/index.html.fb4e8b68.js"><link rel="prefetch" href="/assets/index.html.a04dd3d3.js"><link rel="prefetch" href="/assets/index.html.ff009d80.js"><link rel="prefetch" href="/assets/index.html.bf8f59c9.js"><link rel="prefetch" href="/assets/index.html.d96055c1.js"><link rel="prefetch" href="/assets/index.html.2126153c.js"><link rel="prefetch" href="/assets/index.html.f45aae34.js"><link rel="prefetch" href="/assets/index.html.7f48a75e.js"><link rel="prefetch" href="/assets/index.html.07f86cc2.js"><link rel="prefetch" href="/assets/index.html.cf475022.js"><link rel="prefetch" href="/assets/index.html.881f858e.js"><link rel="prefetch" href="/assets/index.html.efd3a267.js"><link rel="prefetch" href="/assets/index.html.8111aed0.js"><link rel="prefetch" href="/assets/index.html.76958b33.js"><link rel="prefetch" href="/assets/index.html.a4fddea4.js"><link rel="prefetch" href="/assets/index.html.f7d705bb.js"><link rel="prefetch" href="/assets/index.html.f55a68b7.js"><link rel="prefetch" href="/assets/index.html.7f012b50.js"><link rel="prefetch" href="/assets/index.html.c0ba8663.js"><link rel="prefetch" href="/assets/index.html.78a29513.js"><link rel="prefetch" href="/assets/index.html.fb327772.js"><link rel="prefetch" href="/assets/index.html.bbc9b385.js"><link rel="prefetch" href="/assets/index.html.ea22432c.js"><link rel="prefetch" href="/assets/index.html.140ed163.js"><link rel="prefetch" href="/assets/index.html.503474a9.js"><link rel="prefetch" href="/assets/index.html.deae5fb3.js"><link rel="prefetch" href="/assets/offer.html.ff9b1926.js"><link rel="prefetch" href="/assets/resume.html.33ad998c.js"><link rel="prefetch" href="/assets/deeplearning.html.916e4670.js"><link rel="prefetch" href="/assets/about.html.3a0cef4b.js"><link rel="prefetch" href="/assets/index.html.0ba4be57.js"><link rel="prefetch" href="/assets/cpu.html.0eb453d0.js"><link rel="prefetch" href="/assets/process.html.a9d54a52.js"><link rel="prefetch" href="/assets/mysql.html.9b06278e.js"><link rel="prefetch" href="/assets/obsidian.html.484cd36e.js"><link rel="prefetch" href="/assets/DiffusionModel.html.51e48802.js"><link rel="prefetch" href="/assets/index.html.8150c453.js"><link rel="prefetch" href="/assets/basic.html.0599d6c7.js"><link rel="prefetch" href="/assets/classification.html.eb661d62.js"><link rel="prefetch" href="/assets/classifier.html.2f8e596a.js"><link rel="prefetch" href="/assets/cnn.html.e96ca693.js"><link rel="prefetch" href="/assets/gan.html.2779a2f6.js"><link rel="prefetch" href="/assets/FaceAssign.html.c4980493.js"><link rel="prefetch" href="/assets/Initialization.html.3f72863c.js"><link rel="prefetch" href="/assets/MetricLearning.html.b1245523.js"><link rel="prefetch" href="/assets/ema.html.0b9005a0.js"><link rel="prefetch" href="/assets/mlp.html.bb558cc6.js"><link rel="prefetch" href="/assets/nlp.html.9e7d231b.js"><link rel="prefetch" href="/assets/norm.html.4430c082.js"><link rel="prefetch" href="/assets/object_dection.html.9dfa3aec.js"><link rel="prefetch" href="/assets/fine_tune.html.a2f4e93b.js"><link rel="prefetch" href="/assets/pytorch-install.html.8c45f3cd.js"><link rel="prefetch" href="/assets/train.html.927381a1.js"><link rel="prefetch" href="/assets/transformer.html.5af99d6c.js"><link rel="prefetch" href="/assets/index.html.89a3efab.js"><link rel="prefetch" href="/assets/2019ICML_Zhang_TRADES.html.28bdde51.js"><link rel="prefetch" href="/assets/2019_Shafahi_LS.html.f9ba8a17.js"><link rel="prefetch" href="/assets/2020ICML_Rice_RobustOverfitting.html.9d9dab14.js"><link rel="prefetch" href="/assets/2020NIPS_Pang_HE.html.d991366b.js"><link rel="prefetch" href="/assets/2021CVPR_Luo_Diffusion3DPointCloud.html.56aa984f.js"><link rel="prefetch" href="/assets/2021ICML_radford_CLIP.html.cb6a2a5b.js"><link rel="prefetch" href="/assets/AdversarialTraining.html.41f011e0.js"><link rel="prefetch" href="/assets/StyleGAN2.html.9bc10f1c.js"><link rel="prefetch" href="/assets/latex.html.6d8eebeb.js"><link rel="prefetch" href="/assets/backtracking.html.513a76ec.js"><link rel="prefetch" href="/assets/dp.html.494f9878.js"><link rel="prefetch" href="/assets/monotonic_stack.html.add5e854.js"><link rel="prefetch" href="/assets/wuhan23_sunset.html.fc258c63.js"><link rel="prefetch" href="/assets/多卡训练.html.4c8e589a.js"><link rel="prefetch" href="/assets/index.html.6a5ed9fa.js"><link rel="prefetch" href="/assets/conda.html.eb241059.js"><link rel="prefetch" href="/assets/CXX.html.10ab9b31.js"><link rel="prefetch" href="/assets/deploy.html.f2e1c53f.js"><link rel="prefetch" href="/assets/mAP.html.a4830371.js"><link rel="prefetch" href="/assets/design_pattern.html.af8253f9.js"><link rel="prefetch" href="/assets/freertos.html.8e6d8054.js"><link rel="prefetch" href="/assets/command.html.95771a02.js"><link rel="prefetch" href="/assets/git-zhihu.html.81819e33.js"><link rel="prefetch" href="/assets/git.html.4706fb77.js"><link rel="prefetch" href="/assets/github.html.0c764380.js"><link rel="prefetch" href="/assets/http.html.315590dc.js"><link rel="prefetch" href="/assets/linux-ubuntu.html.9e511335.js"><link rel="prefetch" href="/assets/linux.html.872d0998.js"><link rel="prefetch" href="/assets/macos.html.64ef826c.js"><link rel="prefetch" href="/assets/mosquitto.html.1de18add.js"><link rel="prefetch" href="/assets/index.html.589071eb.js"><link rel="prefetch" href="/assets/openvino.html.eca69b51.js"><link rel="prefetch" href="/assets/list.html.8c6c9d6b.js"><link rel="prefetch" href="/assets/python.html.f9ba2225.js"><link rel="prefetch" href="/assets/OpenWRT.html.2986b654.js"><link rel="prefetch" href="/assets/homeAssistant-zhihu.html.ed8e4b69.js"><link rel="prefetch" href="/assets/homeAssistant.html.ea1ee46f.js"><link rel="prefetch" href="/assets/icm20602.html.9f687a37.js"><link rel="prefetch" href="/assets/raspberrypi.html.021e2a37.js"><link rel="prefetch" href="/assets/server.html.d03beb8a.js"><link rel="prefetch" href="/assets/ros2.html.7a788426.js"><link rel="prefetch" href="/assets/rtmp.html.5729e3de.js"><link rel="prefetch" href="/assets/ssh.html.49d8f0a3.js"><link rel="prefetch" href="/assets/Vue-init.html.a7944208.js"><link rel="prefetch" href="/assets/index.html.dd184f3b.js"><link rel="prefetch" href="/assets/CodiMD.html.91815fce.js"><link rel="prefetch" href="/assets/PromptEngineering.html.4a540894.js"><link rel="prefetch" href="/assets/QQ-GPT.html.132edc13.js"><link rel="prefetch" href="/assets/utm串口挂载.html.5a6dd758.js"><link rel="prefetch" href="/assets/BlenderPython.html.07992b3f.js"><link rel="prefetch" href="/assets/ContentAuthoring-vehicles.html.5ebe06be.js"><link rel="prefetch" href="/assets/apis.html.d696b5f0.js"><link rel="prefetch" href="/assets/carla-zhihu.html.be4dc108.js"><link rel="prefetch" href="/assets/carla.html.858323b9.js"><link rel="prefetch" href="/assets/python-api.html.18f282d1.js"><link rel="prefetch" href="/assets/retrieve_data.html.582e301d.js"><link rel="prefetch" href="/assets/clash.html.c67b2826.js"><link rel="prefetch" href="/assets/pytorch3d.html.5a795719.js"><link rel="prefetch" href="/assets/vscode.html.9b1ffcac.js"><link rel="prefetch" href="/assets/start.html.b835c286.js"><link rel="prefetch" href="/assets/dataset.html.6f0d8294.js"><link rel="prefetch" href="/assets/index.html.949ab12f.js"><link rel="prefetch" href="/assets/diffusion_attack.html.29de8087.js"><link rel="prefetch" href="/assets/cuda_cudnn.html.f5cc355d.js"><link rel="prefetch" href="/assets/TensorRT.legacy.html.a76805a3.js"><link rel="prefetch" href="/assets/TensorRT.html.ba22a425.js"><link rel="prefetch" href="/assets/cuda.html.0488f309.js"><link rel="prefetch" href="/assets/index.html.9bbc9453.js"><link rel="prefetch" href="/assets/JetsonXavierNX-zhihu.html.d5378e3e.js"><link rel="prefetch" href="/assets/jetson_xavier.html.2412768c.js"><link rel="prefetch" href="/assets/basic.html.3f8cc93d.js"><link rel="prefetch" href="/assets/cmake.html.40db936e.js"><link rel="prefetch" href="/assets/cxx11.html.bb03bba9.js"><link rel="prefetch" href="/assets/hash_table.html.9d5bfbbb.js"><link rel="prefetch" href="/assets/tree.html.cf005b44.js"><link rel="prefetch" href="/assets/generics.html.798f81ee.js"><link rel="prefetch" href="/assets/lambda.html.8e0ffb3c.js"><link rel="prefetch" href="/assets/rvalue.html.fc85ec05.js"><link rel="prefetch" href="/assets/safety_efficiency.html.7fb43c7b.js"><link rel="prefetch" href="/assets/smart_ptr.html.db4d2752.js"><link rel="prefetch" href="/assets/std_cincout.html.3a158269.js"><link rel="prefetch" href="/assets/stl.html.10562152.js"><link rel="prefetch" href="/assets/thread.html.0097da8a.js"><link rel="prefetch" href="/assets/trick.html.2bcd0da9.js"><link rel="prefetch" href="/assets/java-env.html.f87c01ed.js"><link rel="prefetch" href="/assets/basic.html.dfec0091.js"><link rel="prefetch" href="/assets/linux-basic.html.f69a24c1.js"><link rel="prefetch" href="/assets/linux-file_system.html.b65414b5.js"><link rel="prefetch" href="/assets/实验1 Linux.html.f0ec5293.js"><link rel="prefetch" href="/assets/实验2 Makefile实验.html.c6386523.js"><link rel="prefetch" href="/assets/cp.html.90826f40.js"><link rel="prefetch" href="/assets/frp.html.b702f983.js"><link rel="prefetch" href="/assets/tmux.html.0e490e1b.js"><link rel="prefetch" href="/assets/ubuntu.html.be8b3cb6.js"><link rel="prefetch" href="/assets/vim.html.24350952.js"><link rel="prefetch" href="/assets/iterator.html.5f7e746c.js"><link rel="prefetch" href="/assets/list.html.1f38d27e.js"><link rel="prefetch" href="/assets/icm20602.html.4ce3bb2f.js"><link rel="prefetch" href="/assets/404.html.19457a36.js"><link rel="prefetch" href="/assets/index.html.3c6db038.js"><link rel="prefetch" href="/assets/index.html.dfd177c1.js"><link rel="prefetch" href="/assets/index.html.1fefafe7.js"><link rel="prefetch" href="/assets/index.html.3ba861ca.js"><link rel="prefetch" href="/assets/index.html.110f72ae.js"><link rel="prefetch" href="/assets/index.html.2d7ddcf7.js"><link rel="prefetch" href="/assets/index.html.061e6056.js"><link rel="prefetch" href="/assets/index.html.70e8c913.js"><link rel="prefetch" href="/assets/index.html.649a7ed6.js"><link rel="prefetch" href="/assets/index.html.3541f6bb.js"><link rel="prefetch" href="/assets/index.html.e8ea8643.js"><link rel="prefetch" href="/assets/index.html.dc99c5bc.js"><link rel="prefetch" href="/assets/index.html.170cc290.js"><link rel="prefetch" href="/assets/index.html.8e9fb1c8.js"><link rel="prefetch" href="/assets/index.html.5ddf73bd.js"><link rel="prefetch" href="/assets/index.html.9a090320.js"><link rel="prefetch" href="/assets/index.html.2ea925fa.js"><link rel="prefetch" href="/assets/index.html.abc694ac.js"><link rel="prefetch" href="/assets/index.html.29eb2768.js"><link rel="prefetch" href="/assets/index.html.95048c64.js"><link rel="prefetch" href="/assets/index.html.3ce1f8fc.js"><link rel="prefetch" href="/assets/index.html.9475c23e.js"><link rel="prefetch" href="/assets/index.html.05ea4196.js"><link rel="prefetch" href="/assets/index.html.674424db.js"><link rel="prefetch" href="/assets/index.html.135a28c3.js"><link rel="prefetch" href="/assets/index.html.2eeb9f50.js"><link rel="prefetch" href="/assets/index.html.aabe3b99.js"><link rel="prefetch" href="/assets/index.html.3a156cfd.js"><link rel="prefetch" href="/assets/index.html.29d8b8e9.js"><link rel="prefetch" href="/assets/index.html.7be77f13.js"><link rel="prefetch" href="/assets/404.cf2ec252.js"><link rel="prefetch" href="/assets/HomePage.ecf27057.js"><link rel="prefetch" href="/assets/Layout.719a42c1.js"><link rel="prefetch" href="/assets/Links.39dae116.js"><link rel="prefetch" href="/assets/Post.7a480b17.js"><link rel="prefetch" href="/assets/Tags.9be1392a.js">
    <link rel="stylesheet" href="/assets/style.fd106935.css">
  </head>
  <body>
    <div id="app"><!--[--><div class="theme-container"><!--[--><header class="navbar is-fixed is-visible invert"><span><a href="/" class=""><span class="site-name">cd Home/主页</span></a></span><div class="navbar-items-wrapper" style=""><!--[--><!--]--><nav class="navbar-items can-hide"><!--[--><div class="navbar-item"><a href="/" class="" aria-label="主页/Home"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="0.48 0.48 23.04 23.04" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"/></svg></span><span>主页/Home</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/links/" class="" aria-label="文章分类/Links"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M305.45 462.59c7.391 7.298 6.188 20.097-3 25.004-77.714 41.802-176.726 29.91-242.344-35.709-65.602-65.603-77.51-164.523-35.692-242.331 4.891-9.095 17.69-10.298 25.003-3l116.812 116.813 27.394-27.394c-.688-2.61-1.594-5.001-1.594-7.814a32.004 32.004 0 1132.004 32.005c-2.797 0-5.204-.891-7.798-1.594l-27.41 27.41zm206.526-159.523a16.103 16.103 0 01-16.002 17.003H463.86a15.97 15.97 0 01-15.892-15.002C440.467 175.549 336.453 70.534 207.03 63.533a15.845 15.845 0 01-15.002-15.908V16.027A16.094 16.094 0 01209.03.024C372.255 8.62 503.475 139.841 511.976 303.067zm-96.012-.297a16.21 16.21 0 01-16.112 17.3h-32.207a16.069 16.069 0 01-15.893-14.705c-6.907-77.011-68.118-138.91-144.924-145.224a15.94 15.94 0 01-14.8-15.893v-32.114a16.134 16.134 0 0117.3-16.096c110.123 8.501 198.228 96.607 206.636 206.732z"/></svg></span><span>文章分类/Links</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/tags/" class="" aria-label="标签/Tags"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg></span><span>标签/Tags</span><!--[--><!--]--></a></div><!--]--><div class="navbar-item"><a style="cursor:pointer;"><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M11 2c4.968 0 9 4.032 9 9s-4.032 9-9 9-9-4.032-9-9 4.032-9 9-9zm0 16c3.867 0 7-3.133 7-7 0-3.868-3.133-7-7-7-3.868 0-7 3.132-7 7 0 3.867 3.132 7 7 7zm8.485.071l2.829 2.828-1.415 1.415-2.828-2.829 1.414-1.414z"/></svg></span><span>Search</span></a></div></nav><!--[--><!--]--><!----></div></header><!--]--><div class="sidebar-mask"></div><!--[--><aside class="sidebar"><nav class="navbar-items"><!--[--><div class="navbar-item"><a href="/" class="" aria-label="主页/Home"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="0.48 0.48 23.04 23.04" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"/></svg></span><span>主页/Home</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/links/" class="" aria-label="文章分类/Links"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M305.45 462.59c7.391 7.298 6.188 20.097-3 25.004-77.714 41.802-176.726 29.91-242.344-35.709-65.602-65.603-77.51-164.523-35.692-242.331 4.891-9.095 17.69-10.298 25.003-3l116.812 116.813 27.394-27.394c-.688-2.61-1.594-5.001-1.594-7.814a32.004 32.004 0 1132.004 32.005c-2.797 0-5.204-.891-7.798-1.594l-27.41 27.41zm206.526-159.523a16.103 16.103 0 01-16.002 17.003H463.86a15.97 15.97 0 01-15.892-15.002C440.467 175.549 336.453 70.534 207.03 63.533a15.845 15.845 0 01-15.002-15.908V16.027A16.094 16.094 0 01209.03.024C372.255 8.62 503.475 139.841 511.976 303.067zm-96.012-.297a16.21 16.21 0 01-16.112 17.3h-32.207a16.069 16.069 0 01-15.893-14.705c-6.907-77.011-68.118-138.91-144.924-145.224a15.94 15.94 0 01-14.8-15.893v-32.114a16.134 16.134 0 0117.3-16.096c110.123 8.501 198.228 96.607 206.636 206.732z"/></svg></span><span>文章分类/Links</span><!--[--><!--]--></a></div><div class="navbar-item"><a href="/tags/" class="" aria-label="标签/Tags"><!--[--><!--]--><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M0 252.118V48C0 21.49 21.49 0 48 0h204.118a48 48 0 0133.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882L293.823 497.941c-18.745 18.745-49.137 18.745-67.882 0L14.059 286.059A48 48 0 010 252.118zM112 64c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48z"/></svg></span><span>标签/Tags</span><!--[--><!--]--></a></div><!--]--><div class="navbar-item"><a style="cursor:pointer;"><span class="nav-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M11 2c4.968 0 9 4.032 9 9s-4.032 9-9 9-9-4.032-9-9 4.032-9 9-9zm0 16c3.867 0 7-3.133 7-7 0-3.868-3.133-7-7-7-3.868 0-7 3.132-7 7 0 3.867 3.132 7 7 7zm8.485.071l2.829 2.828-1.415 1.415-2.828-2.829 1.414-1.414z"/></svg></span><span>Search</span></a></div></nav><!--[--><!--]--><ul class="sidebar-items"><!--[--><li><p tabindex="0" class="sidebar-item sidebar-heading">论文 <!----></p><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#工具" class="router-link-active router-link-exact-active sidebar-item" aria-label="工具"><!--[--><!--]--><!----><span>工具</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#润色段落" class="router-link-active router-link-exact-active sidebar-item" aria-label="润色段落"><!--[--><!--]--><!----><span>润色段落</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#abstract" class="router-link-active router-link-exact-active sidebar-item" aria-label="Abstract"><!--[--><!--]--><!----><span>Abstract</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#intro" class="router-link-active router-link-exact-active sidebar-item" aria-label="Intro"><!--[--><!--]--><!----><span>Intro</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#related-work" class="router-link-active router-link-exact-active sidebar-item" aria-label="Related Work"><!--[--><!--]--><!----><span>Related Work</span><!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#物理攻击" class="router-link-active router-link-exact-active sidebar-item" aria-label="物理攻击"><!--[--><!--]--><!----><span>物理攻击</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#图像修复-纹理ae" class="router-link-active router-link-exact-active sidebar-item" aria-label="图像修复（纹理AE）"><!--[--><!--]--><!----><span>图像修复（纹理AE）</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#gan-生成对抗样本的" class="router-link-active router-link-exact-active sidebar-item" aria-label="GAN 生成对抗样本的"><!--[--><!--]--><!----><span>GAN 生成对抗样本的</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#解耦表征学习" class="router-link-active router-link-exact-active sidebar-item" aria-label="解耦表征学习"><!--[--><!--]--><!----><span>解耦表征学习</span><!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#参考文献" class="router-link-active router-link-exact-active sidebar-item" aria-label="参考文献"><!--[--><!--]--><!----><span>参考文献</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#训练过程" class="router-link-active router-link-exact-active sidebar-item" aria-label="训练过程"><!--[--><!--]--><!----><span>训练过程</span><!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#loss-为-nan" class="router-link-active router-link-exact-active sidebar-item" aria-label="loss 为 NAN"><!--[--><!--]--><!----><span>loss 为 NAN</span><!--[--><!--]--></a><!----></li><!--]--></ul></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#关于-add-和-concat" class="router-link-active router-link-exact-active sidebar-item" aria-label="关于 add 和 concat"><!--[--><!--]--><!----><span>关于 add 和 concat</span><!--[--><!--]--></a><!----></li><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#其他" class="router-link-active router-link-exact-active sidebar-item" aria-label="其他"><!--[--><!--]--><!----><span>其他</span><!--[--><!--]--></a><ul style="" class="sidebar-item-children"><!--[--><li><a aria-current="page" href="/Reseach/myResearch/%E5%AF%B9%E6%8A%97%E7%BA%B9%E7%90%86.html#_3d-model" class="router-link-active router-link-exact-active sidebar-item" aria-label="3D model"><!--[--><!--]--><!----><span>3D model</span><!--[--><!--]--></a><!----></li><!--]--></ul></li><!--]--></ul></li><!--]--></ul><!--[--><!--]--></aside><!--]--><div class="page-content"><!--[--><main class="page"><!--[--><div class="article-header" style=""><!----><div class="article-header-content"><!----><h1 class="article-title"></h1><!----><div class="article-icons"><div class="article-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M313.6 304c-28.7 0-42.5 16-89.6 16-47.1 0-60.8-16-89.6-16C60.2 304 0 364.2 0 438.4V464c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48v-25.6c0-74.2-60.2-134.4-134.4-134.4zM400 464H48v-25.6c0-47.6 38.8-86.4 86.4-86.4 14.6 0 38.3 16 89.6 16 51.7 0 74.9-16 89.6-16 47.6 0 86.4 38.8 86.4 86.4V464zM224 288c79.5 0 144-64.5 144-144S303.5 0 224 0 80 64.5 80 144s64.5 144 144 144zm0-240c52.9 0 96 43.1 96 96s-43.1 96-96 96-96-43.1-96-96 43.1-96 96-96z"/></svg><span>Henry Zhu</span></div><!----><div class="article-icon"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="0 0 24 24" fill="currentColor"><path fill="none" d="M0 0h24v24H0z"/><path d="M17.618 5.968l1.453-1.453 1.414 1.414-1.453 1.453a9 9 0 11-1.414-1.414zM12 20a7 7 0 100-14 7 7 0 000 14zM11 8h2v6h-2V8zM8 1h8v2H8V1z"/></svg><span>27 min</span></div></div></div><!----></div><!--[--><!--]--><!--]--><div class="theme-gungnir-content"><!--[--><!--]--><div><h1 id="论文" tabindex="-1"><a class="header-anchor" href="#论文" aria-hidden="true">#</a> 论文</h1><h2 id="工具" tabindex="-1"><a class="header-anchor" href="#工具" aria-hidden="true">#</a> 工具</h2><p><a href="https://cloudconvert.com/pdf-to-eps" target="_blank" rel="noopener noreferrer">pdf 转 eps</a></p><h2 id="润色段落" tabindex="-1"><a class="header-anchor" href="#润色段落" aria-hidden="true">#</a> 润色段落</h2><p>Can you help me introduce the Representation Learning to make it look like it was written by a native English speaker. You must use</p><p>Can you help me polish the following paragraph to make it look like it was written by a native English speaker: Our goal is to generate a realistic texture camouflage to successfully attack the target detector.</p><ul><li><strong>论文写作</strong> :You act as an academic journal editor of IEEE. Please correct grammar errors if possiable, and then improve, rephrase and polish the following academic paragraph from an academic angleto improve their academic quality and grammatical structure:</li></ul><p>You act as an academic journal editor of IEEE. I will provide you multiple paragraph from an academic paper. Polish the writing to meet the academic style, improve the spelling, grammar, clarity, concision and overall readability. When necessary, rewrite the whole sentence. Furthermore, list all modification and explain the reasons to do so in markdown table.</p><p>Now I will provide paragraph:</p><p>While existing substantial research on digital attacks, their practical application often require the access to modify the entire digital images, which is extemely impractical in the real physical scenarios~\cite{2018_Liu_AdvDetPatch} for the reason that the perturbations cannot be casually added to the real-world environments. Therefore, this article mainly focuses on physical attacks.</p><p>伪装成编辑 I want you to act as an academic journal editor of IEEE. Please translate below paragraph to English and rephrase the paragraph from an academic angle based on the writting style of the Elsevier&#39;s Information Sciences journal: 与数字世界在整张图像上添加扰动不同，物理世界仅能通过在目标物体上进行贴图或者修改目标物体的形状、纹理等特征产生物理对抗样本，因此物理世界的对抗样本更具备实用性和现实意义，同时也意味着产生对抗样本更加具有挑战性。</p><ul><li><strong>Translation Prompt（翻译）</strong>:I would like you to serve as an English translator, proofreader, and editor to translate my upcoming Chinese content into elegant, refined, and academic English. Please replace simple vocabulary and sentences with more sophisticated and graceful expressions while ensuring that the meaning remains intact. Overall, the language style should be similar to the <em>American Economic</em> Review academic journal. If you understand, please provide an example first.</li><li><strong>Editing Prompt（润色）</strong>:Please reword and refine these paragraphs to improve their academic quality and grammatical structure. Ideally, the language should be similar to that found in the <em>American Economic</em> Review. Please note all the changes made.Please use the language style found in the <em>American Economic</em> Review to improve the grammar and language of the paragraphs.Please act as an academic journal editor and rewrite the paragraph from an academic perspective, using the writing style of the Nature journal.</li></ul><blockquote><p>一般至少要经过两轮翻译和编辑才能使语言稍微好一点。</p></blockquote><p>多角度一致性的漏洞挖掘方法：</p><p>不同观察视角下，物理伪装的方法都能有效地误导模型产生错误的类别预测。但是，大多数的对抗伪装会导致模型产生不一致的输出，从而可能触发严重的安全警告。 模型对不同的类别预测能力不一致是这种不一致性产生的主要原因。因此，挖掘模型对不同类别的识别能力，提升攻击的一致性，从而挖掘出模型自身的漏洞至关重要。为此，我们提出基于语义引导的一致性的对抗伪装生成方法：通过在隐空间下对伪装进行语义约束，使得（即使在不同视角下）伪装的语义信息与指定的的语义保持一致，以挖掘模型对于不同类别的误识别能力，提高攻击算法一致性的同时也能更好挖掘模型潜在的漏洞。 目前，我们在自动驾驶模拟环境carla中已经完成完成了理论验证和测试，正在物理世界中进行迁移实验验证。</p><h2 id="abstract" tabindex="-1"><a class="header-anchor" href="#abstract" aria-hidden="true">#</a> Abstract</h2><p>在真实世界中，物理的对抗攻击通过贴图或者伪装的方法，能非常有效地欺骗目标检测模型，以使得目标检测模型产生错误的类别预测。 In the real world, physical adversarial attacks have exhibited a remarkable capability of fooling object detection models, leading to incorrect predictions regarding categories or bounding boxes.</p><p>在不同的观察视角下，基于伪装的物理攻击都能够有效地成功攻击目标检测模型。 但是，大多数的对抗伪装，在上述情况下，通常会导致目标检测模型产生不一致的预测输出，从而可能触发严重的安全警告。 这样的现象凸显了以前物理对抗伪装方法的较差的隐蔽性。 Among physical attacks, camouflage-based physical adversarial attacks are particularly effective at fooling object detection models, even from different obversing viewpoints. Despite this, a significant characteristic of existing adversarial camouflages is their multi-view inconsistency in attack results. More specifically, the predicted category will frequently change as the observing view changes. This phenomenon may trigger serious security warnings and hence highlights the limited stealthiness of previous physical adversarial camouflage methods.</p><p>然而，我们认为，基于伪装的对抗攻击完全可以做到攻击结果的一致性，以实现更好的隐蔽性，从而对机器视觉系统的安全性造成更大的危害。 为了证明我们的观点，我们提出了在隐空间下的基于语义引导的一致性的对抗伪装生成方法。 该生成方法通过在隐空间下对对抗伪装进行语义约束，使得伪装的语义信息与指定的的语义信息保持一致性，即使是在不同视角下该一致性也能保持，从而生成的对抗性伪装具备更好的隐蔽性。 Nevertheless, we believe that camouflage-based adversarial attacks can indeed achieve consistency of attack results to improve stealthiness, thereby causing serious threat to the security of machine vision systems. To prove our point, we propose a novel approach for consistent adversarial camouflage generation through semantic guidance in the latent space. Our approach enhances semantic constraints on adversarial camouflage in the latent space, thereby ensuring that the semantic information of the camouflage remains aligned with the pre-specified categories, regardless of from which view. This enhancement ultimately results in adversarial camouflage exhibiting better stealthiness.</p><p>我们在虚拟的物理世界中进行了针对 SOTA 的目标检测模型的攻击实验，实验结果表明我们的方法在不仅具备了很好的攻击一致性，也有较好的攻击性能，使得我们的攻击具备较好的隐蔽性 We conducted various attack experiments against state-of-the-art object detection models within the simulated physical world. The experimental results demonstrate that our method not only exhibits notable attack consistency but also delivers excellent attack performance.</p><h2 id="intro" tabindex="-1"><a class="header-anchor" href="#intro" aria-hidden="true">#</a> Intro</h2><p>【①】第一句写任务介绍以及研究意义 （简述）深度学习成就。（引出）对抗样本的研究背景。对抗样本的研究可以帮助更好地理解深度学习的原理和弱点，指导人们设计更加安全鲁棒的深度学习系统。尤其是在自动驾驶领域</p><p>【②】第二句概述研究现状以及存在主要的问题 对抗样本的研究分为数字/物理攻击，数字攻击对于人们研究深度学习弱点有重要意义；而物理攻击将数字攻击中的原理迁移到真实世界的实际应用中，主要攻击目标检测器，需要考虑实际环境、背景、采集设备的影响，对于指导人们设计更加安全鲁棒的深度学习系统有重要意义。 物理攻击分为 patch/camouflage-based： patch 的攻击是只能应用于平面和刚性物体，只能在特定视角才能生效，攻击应用受到限制，例对于多视角下的如无人机、监控领域。近年来，camouflage 引起了越来越多的关注，就通过对于物体本身纹理的修改，考虑了物体本身的形状，同时这种攻击更好地考虑到了物理的变化（位置、方向等），因此通常能在不同视角、不同背景下都有较好的效果。</p><p>【②】第二句概述研究现状以及存在主要的问题 (数字/物理攻击分类，分为 patch/camouflage) 但是伪装的方法的问题，不同视角下的攻击结果不一致，每次都会产生不同的识别结果，触发安全警告，导致攻击的隐蔽性很差。（例如无人机在环绕目标进行跟踪识别的场景下）</p><p>【③】第三句写解决这些问题的研究挑战 以前的工作旨在提高对抗伪装的攻击成功率，忽略了伪装的具体语义信息，因此无法保证攻击结果的一致性。 而产生特定语义的伪装并不是一件容易的事情，因为在伪装在不同视角、环境下都是不一致的，需要在保证多视角下的攻击成功的同时，保证伪装的语义一致性。</p><p>【④】第四句写当前方法的主要出发点以及解决思路 为了实现不同角度的攻击一致性，我们提出了一种纹理生成方法，通过引入类别信息，对纹理进行编辑，生成具有特定语义信息的伪装，以实现语义一致性。</p><p>【⑤】第五句写当前方法的主要技术方案 1、自编码器进行纹理映射 2、将类别信息引入纹理生成过程中，引导生产具有特定语义信息的伪装</p><p>【⑥】第六句写总结、强调贡献 1、通过自编码器减少纹理生成的计算量。 2、生成式的方法进行对抗纹理生成，而不只局限于梯度优化的方式，增加了纹理生成方法的多样性 3、通过引入类别信息，生成具有特定语义信息的伪装，提高攻击的稳定性</p><p>【③】第三句写解决这些问题的研究挑战 但是伪装攻击仍然存在问题： 1、伪装的目标形状不一致，不同的方法，不一定能进行迁移。 2、以往的研究，通过梯度优化的方式直接修改纹理，但是如果对于纹理数量较多的情况，需要优化梯度所需要的计算量就非常大了，尤其是全覆盖伪装的方式，需要修改的纹理数量更多，计算量更大。并且只能无法将一些效果好的应用在图像上的算法应用进来。 3、伪装受限于环境，在不同的环境下，效果不一定好。伪装的攻击效果不稳定，每次都会产生不同的结果，可能会触发安全警告。</p><p>【④】第四句写当前方法的主要出发点以及解决思路 1、我们将伪装的纹理映射至 latent ，一方面可以解决计算量的问题，另一方面可以应用 latent editing 技术对于纹理进行修改，从而可以将一些效果好的应用在图像上的算法应用进来。 2、将类别信息引入对抗生成中，通过 latent editing 技术，生成指定类别的纹理，从而产生稳定的攻击效果</p><p>【⑤】第五句写当前方法的主要技术方案 1、自编码器进行纹理映射 2、借鉴GAN的思想，将类别信息引入对抗生成中，引导生产具有特定语义信息的伪装</p><p>【⑥】第六句写总结、强调贡献 1、通过自编码器减少纹理生成的计算量。 2、生成式的方法进行对抗纹理生成，而不只局限于梯度优化的方式，增加了纹理生成方法的多样性 3、通过引入类别信息，生成具有特定语义信息的伪装，提高攻击的稳定性</p><h2 id="related-work" tabindex="-1"><a class="header-anchor" href="#related-work" aria-hidden="true">#</a> Related Work</h2><h3 id="物理攻击" tabindex="-1"><a class="header-anchor" href="#物理攻击" aria-hidden="true">#</a> 物理攻击</h3><p>以往的攻击大都是优化一个目标，这种方法效果好，但是产生一个纹理，缺乏多样性 而我们的方法是训练一个纹理生成器，生成多样的对抗纹理，这就使得在实际应用中，可以得到不同的纹理，以针对不同的检测器进行攻击</p><p>车牌 修改纹理 Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector.</p><p>such as wearable t-shirts [19,42,45,47], eye-glass frames [38–40], or car license plates [48]), or static with respect to the scene (such as stickers [6,14,30], posters [26], and traffic signs [8,36,42]</p><h3 id="图像修复-纹理ae" tabindex="-1"><a class="header-anchor" href="#图像修复-纹理ae" aria-hidden="true">#</a> 图像修复（纹理AE）</h3><p><a href="https://www.zhihu.com/question/56801298/answer/155891603" target="_blank" rel="noopener noreferrer">结合深度学习的图像修复怎么实现？ - QZhang的回答 - 知乎</a></p><ul><li><ol start="2"><li>CVPR 2017的High Resolution Inpainting(Context-Encoders+CNNMRF框架)，结合了风格迁移的思路，链接： <a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.09969" target="_blank" rel="noopener noreferrer">High-Resolution Image Inpainting using Multi-Scale Neural Patch Synthesis</a>; <a href="https://link.zhihu.com/?target=https%3A//github.com/leehomyc/Faster-High-Res-Neural-Inpainting" target="_blank" rel="noopener noreferrer">Github代码</a></li></ol></li><li><ol start="4"><li>SIGGRAPH 2017 (ACM ToG)的<a href="https://link.zhihu.com/?target=http%3A//hi.cs.waseda.ac.jp/~iizuka/projects/completion/en/" target="_blank" rel="noopener noreferrer">Globally and Locally Consistent Image Completion</a> (CE中加入Global+Local两个判别器的改进)， <a href="https://link.zhihu.com/?target=https%3A//github.com/satoshiiizuka/siggraph2017_inpainting" target="_blank" rel="noopener noreferrer">Github代码</a></li></ol></li><li>ICCV 2019的<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1905.12384" target="_blank" rel="noopener noreferrer">Coherent Semantic Attention for Image Inpainting</a>，论文作者为Kuma , 文中提出了一个全新的Attention模块，该模块不仅有效的利用了上下文信息同时能够捕捉到生成补丁之间的相关性。同时提出了一个新的损失函数配合模块的工作，最后利用一个新的特征感知辨别器对细节效果进行加强，<a href="https://github.com/KumapowerLIU/CSA-inpainting" target="_blank" rel="noopener noreferrer">代码过段时间会公开</a>。</li></ul><h3 id="gan-生成对抗样本的" tabindex="-1"><a class="header-anchor" href="#gan-生成对抗样本的" aria-hidden="true">#</a> GAN 生成对抗样本的</h3><ul><li><p><strong>(AdvGAN)Generating Adversarial Examples with Adversarial Networks (IJCAI 2018)</strong>: <a href="https://arxiv.org/abs/1801.02610" target="_blank" rel="noopener noreferrer">arxiv</a>, <a href="https://github.com/mathcbc/advGAN_pytorch" target="_blank" rel="noopener noreferrer">code</a></p></li><li><p><strong>AdvGAN++: Harnessing latent layers for adversary generation (ICCV 2019)</strong>:<a href="https://arxiv.org/abs/1908.00706" target="_blank" rel="noopener noreferrer">arxiv</a></p></li><li><p><strong>(NaturalGAN)Generating Natural Adversarial Examples (ICLR 2018)</strong>: <a href="https://arxiv.org/abs/1710.11342" target="_blank" rel="noopener noreferrer">arxiv</a>, <a href="https://github.com/zhengliz/natural-adversary" target="_blank" rel="noopener noreferrer">code</a></p></li><li><p><strong>Rob-GAN: Generator, Discriminator, and Adversarial Attacker (CVPR 2019)</strong>: <a href="https://arxiv.org/abs/1807.10454" target="_blank" rel="noopener noreferrer">arxiv</a>, <a href="https://github.com/xuanqing94/RobGAN" target="_blank" rel="noopener noreferrer">code</a></p></li><li><p><strong>AdvGAN++: Harnessing latent layers for adversary generation (ICCV 2019)</strong>: 这篇文章是AdvGAN的改进版本：出发点是latent vector是比image更好的先验，于是把AdvGAN的以image为输入改为以latent vector+noise为输入会生成更好的对抗样本。</p></li><li><p><strong>AI-GAN: Attack-Inspired Generation of Adversarial Examples (ICIP 2021)</strong>: <a href="https://arxiv.org/abs/2002.02196" target="_blank" rel="noopener noreferrer">arvix</a> 本文将target class隐藏到隐变量中，与Natural Generative Perturbation和AdvGAN++的思路很类似。</p></li><li><p><a href="https://arxiv.org/abs/2204.02283" target="_blank" rel="noopener noreferrer">Lost in Latent Space: Disentangled Models and the Challenge of Combinatorial Generalisation</a>(迷失在潜在空间:解耦模型中和组合泛化的挑战)</p></li><li><p><a href="https://arxiv.org/abs/2204.02010" target="_blank" rel="noopener noreferrer">LatentGAN Autoencoder: Learning Disentangled Latent Distribution</a></p></li><li><p>【需要看一下，关于latent的】<a href="https://zhuanlan.zhihu.com/p/140553228" target="_blank" rel="noopener noreferrer">论文解读：InterFaceGAN cvpr2020</a>:本文首先对GAN模型的潜在空间Z中出现的语义属性进行了严格的分析，然后构建了利用潜在编码z中的语义进行面部属性编辑操纵的pipeline。</p></li><li><p>【完全类比了车纹理生成】<a href="https://zhuanlan.zhihu.com/p/381191240" target="_blank" rel="noopener noreferrer">MUST-GAN (cvpr2021)</a>: 本文提出一种多级别统计量迁移模型用于自监督的生成人体图像，该方法可以实现多种<strong>人体生成任务</strong>，包括人体姿态迁移（pose transfer）、人体衣服风格迁移（clothes style transfer）等。同时该方法无需使用人工匹配的图像对（source-target paired images）来进行训练，训练过程以图像自身重构的方式进行，训练集配置成本低，但性能优异，能够媲美有监督人体生成模型的性能。</p></li></ul><p>参考 <a href="https://github.com/Zeleni9/pytorch-wgan" target="_blank" rel="noopener noreferrer">WGAN-GP</a>稳定训练</p><h3 id="解耦表征学习" tabindex="-1"><a class="header-anchor" href="#解耦表征学习" aria-hidden="true">#</a> 解耦表征学习</h3><p><a href="https://zhuanlan.zhihu.com/p/399286095" target="_blank" rel="noopener noreferrer">解耦表征学习 | 领域简述</a></p><p>从生成数据的建模过程理解解耦表征学习：</p><p>在表征学习中, 通常将真实数据 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 的生成过程建模为两部分:</p><ul><li>step1：从先验分布 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span> 中采样得到潜在变量取值</li><li>step2：从条件数据生成分布 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span></span></span></span> 中采样得到数据观测值 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span></li></ul><p>关键性假设：真实数据 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">x</span></span></span></span> 视作由一系列物理语义可解释的因素 <span class="katex"><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">...</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 通过复杂未知的非线性系统映射函数 作用相互耦合生成的 即</p><p>2、解耦表征学习的方法</p><ul><li>VAE 类的方法：β-VAE、factor-VAE、DIP-VAE、β-TCVAE 等</li><li>GAN​ 类的方法：InfoGAN 等</li><li>VAE-GAN 结合的方法</li><li>基于结构化模型先验归纳偏好的方法</li><li>基于物理知识归纳偏好的方法</li></ul><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献" aria-hidden="true">#</a> 参考文献</h2><ul><li><a href="https://www.zhihu.com/question/458164481" target="_blank" rel="noopener noreferrer">如果一个论文只在arXiv上挂着，没有在什么会和期刊发表过，却有几万的引用量，如何评价这类论文？ - 知乎</a></li></ul><h1 id="其他部分" tabindex="-1"><a class="header-anchor" href="#其他部分" aria-hidden="true">#</a> 其他部分</h1><h2 id="训练过程" tabindex="-1"><a class="header-anchor" href="#训练过程" aria-hidden="true">#</a> 训练过程</h2><h3 id="loss-为-nan" tabindex="-1"><a class="header-anchor" href="#loss-为-nan" aria-hidden="true">#</a> loss 为 NAN</h3><p>训练过程中loss为nan的情况，也包括下面几种</p><ul><li>输入中就含有NaN。数据集不完整，例如训练的data或者使用的label为空，就会导致这种情况，label缺失问题也会导致loss一直是nan，需要检查label。</li><li>梯度爆炸，注意每个batch前梯度要清零，optimizer.zero_grad()。</li><li>当我们使用具有 log() 的损失函数，如 Focal Loss 或 Cross Entropy 时，输入张量的某些维度可能是一个非常小的数字，正常情况是float32是一个极其小的数但也不是0，但是amp使用半精度可能就直接摄入到了0，因此就出现NAN的问题了。因此，我们可以在涉及到log 操作的时候，将float16转化为float32</li></ul><h2 id="关于-add-和-concat" tabindex="-1"><a class="header-anchor" href="#关于-add-和-concat" aria-hidden="true">#</a> 关于 add 和 concat</h2><p>作者二：王若霄 concat是肯定是计算量大于element-wise add的，但个人认为，concat避免了直接add对信息造成的负面影响。而且逐元素加和的方式要求不同层的feature map具有完全一致的channel数量，而cancat不受channel数量的限制（yolov3里就是concat，做concat操作的两层通道数不同）</p><p>作者三：Hanjie WU add操作经典代表网络是ResNet,concate操作经典代表网络是Inception系统网络中的Inception结构和DenseNet。</p><p>正如之前的回答有人说的，add操作相当于加入一种先验知识。我觉得也就是相当于你对原始特征进行人为的特征融合。而你选择的特征处理的操作是element-wise add。通过add操作，会得到新的特征，这个新的特征可以反映原始特征的一些特性，但是原始特征的一些信息也会在这个过程中损失。</p><p>但是concate就是将原始特征直接拼接，让网络去学习，应该如何融合特征，这个过程中信息不会损失。</p><p>所以我认为add其实只是concate的一种特殊情况。但是concate带来的计算量较大，在明确原始特征的关系可以使用add操作融合的话，使用add操作可以节省计算代价。</p><p>作者四：店长 我认为没有人能彻底回答这个问题。NN领域很多问题都这样。</p><p>add操作必然会带来信息损失。两头羊加两头羊等于四头羊，这时候加法没有损失信息。但是如果两个被加的向量不具备同类特征含义时怎么办？事实上，如果有了加法操作，训练过程会把加法之后那个特征分解为加法前的两个子特征。如果这两个子特征之和超过了某个阈值，哪怕一正一负，就有激活功能。这两个子特征未必需要是同类特征含义。信息损失如果“损失”得当，就是信息提取！</p><p>NN牛叉就牛叉在这里，说得清的话，早就用特征工程搞定了，还用什么NN。</p><p>作者五：Cyunsiu To add作为一种特征融合的方式在没有时序的网络中是无害的，concat是add的泛华形式，对比densenet和resnet即可知道。</p><p>但是在时序步共享的网络中，add简直是灾难性的，因为时序共享的网络诸如lstm和gru，要求每一个时间步共享参数，那么处在后时序步的网络输出值域要远远大于最开始前几部的值域，所以rnn会有梯度的问题，因此lstm和gru应运而生，不过个人觉得lstm和gru绝对称不上是好网络，解释性太差，每个门的动作仅仅基于启发式的yy来设计，我们需要有能力的学者探讨如何在时序网络中进行特征的融合。</p><p>总结： Resnet是做值的叠加，通道数是不变的，DenseNet是做通道的合并。你可以这么理解，add是描述图像的特征下的信息量增多了，但是描述图像的维度本身并没有增加，只是每一维下的信息量在增加，这显然是对最终的图像的分类是有益的。而concatenate是通道数的合并，也就是说描述图像本身的特征增加了，而每一特征下的信息是没有增加。 ———————————————— 版权声明：本文为CSDN博主「陈知鱼」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。 原文链接：https://blog.csdn.net/weixin_42926076/article/details/100660188</p><h2 id="其他" tabindex="-1"><a class="header-anchor" href="#其他" aria-hidden="true">#</a> 其他</h2><p>随机噪声作为扰动</p><p><img src="/assets/cGAN_overview.4c55c349.jpg" alt=""></p><p>Cross-attention 交叉注意力</p><p><a href="https://zhuanlan.zhihu.com/p/410776234" target="_blank" rel="noopener noreferrer">超详细图解Self-Attention</a></p><p><img src="https://pic1.zhimg.com/v2-941861c16b46d87328a9b50ffdafbf8c_b.jpg" alt="attention"></p><p><a href="https://www.zhihu.com/question/325839123" target="_blank" rel="noopener noreferrer">深度学习attention机制中的Q,K,V分别是从哪来的？</a></p><p><a href="https://www.zhihu.com/question/325839123/answer/2718310467" target="_blank" rel="noopener noreferrer">深度学习attention机制中的Q,K,V分别是从哪来的？</a>A: 理论指导实践:</p><ol><li>对于一个文本，我们希望找到某张图片中和文本描述相关的局部图像，怎么办？文本作query(查询），图像做value（数据库）</li><li>对于一个图像，想要找一个文本中和图像所含内容有关的局部文本，如何设计？图像作query，文本作value.</li><li>自注意力（我查我自己）:我们想知道句子中某个词在整个句子中的分量（或者相关文本），怎么设计？句子本身乘以三个矩阵得到Q,K,V，每个词去查整个句子。</li><li>交叉注意力（查别人）:transformer模型的decoder中，由decoder的输入经过变换作为query，由encoder的输出作为key和value（数据库）。value和query来自不同的地方，就是交叉注意力。可以看到key和value一定是代表着同一个东西。即:[Q,(K,V)]。如果用encoder的输出做value，用decoder的输入做key和query 那就完完全全不make sense了。</li></ol><p>https://zhuanlan.zhihu.com/p/622103480</p><p>本文使用google的Imagen作为扩散模型的backbone。每个扩散步骤 都使用U型网络从噪声图像 和文本嵌入 中预测噪声 。图像和文本两种模态之间的交互发生在噪声预测期间，它们的特征嵌入通过cross-attention层进行融合。</p><p>用数学语言描述，噪声图像的深层特征 被投影到query矩阵 ，文本嵌入被投影到key矩阵 和value矩阵 ，其中 是通过学习的得到的线性投影变换。cross-attention map定义如下：</p><p><a href="https://blog.csdn.net/u010087338/article/details/128622886" target="_blank" rel="noopener noreferrer">图解cross attention</a></p><p><img src="https://img-blog.csdnimg.cn/d4af482b4c1743899cc953aad22aa266.png" alt="Alt text"></p><p>前馈层与交叉注意力相关，除了前馈层确实使用 softmax 并且其中一个输入序列是静态的。 <a href="https://vaclavkosar.com/ml/Feed-Forward-Self-Attendion-Key-Value-Memory" target="_blank" rel="noopener noreferrer">Augmenting Self-attention with Persistent Memory</a> 论文表明，前馈层计算与自注意力相同。</p><p>CA实现： <a href="https://github.com/huggingface/diffusers/blob/4125756e88e82370c197fecf28e9f0b4d7eee6c3/src/diffusers/models/cross_attention.py#L30" target="_blank" rel="noopener noreferrer">huggingface/diffusers</a></p><p><a href="https://www.jianshu.com/p/eb6e886bcdaa" target="_blank" rel="noopener noreferrer">交叉熵、GAN loss与softplus</a></p><h3 id="_3d-model" tabindex="-1"><a class="header-anchor" href="#_3d-model" aria-hidden="true">#</a> 3D model</h3><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Wide-Area_Crowd_Counting_via_Ground-Plane_Density_Maps_and_Multi-View_Fusion_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Wide-Area Crowd Counting via Ground-Plane Density Maps and Multi-View Fusion CNNs</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Yao_Recurrent_MVSNet_for_High-Resolution_Multi-View_Stereo_Depth_Inference_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Recurrent MVSNet for High-Resolution Multi-View Stereo Depth Inference</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Xu_Multi-Scale_Geometric_Consistency_Guided_Multi-View_Stereo_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Multi-Scale Geometric Consistency Guided Multi-View Stereo</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kasten_GPSfM_Global_Projective_SFM_Using_Algebraic_Constraints_on_Multi-View_Fundamental_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View Fundamental Matrices</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kocabas_Self-Supervised_Learning_of_3D_Human_Pose_Using_Multi-View_Geometry_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_MVF-Net_Multi-View_3D_Face_Morphable_Model_Regression_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">MVF-Net: Multi-View 3D Face Morphable Model Regression</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Trager_Coordinate-Free_Carlsson-Weinshall_Duality_and_Relative_Multi-View_Geometry_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Coordinate-Free Carlsson-Weinshall Duality and Relative Multi-View Geometry</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Srinivasan_Pushing_the_Boundaries_of_View_Extrapolation_With_Multiplane_Images_CVPR_2019_paper.pdfhttp://openaccess.thecvf.com/content_CVPR_2019/papers/Srinivasan_Pushing_the_Boundaries_of_View_Extrapolation_With_Multiplane_Images_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Strand-Accurate Multi-View Hair Capture</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lei_Zhou_Learning_and_Matching_ECCV_2018_paper.pdf" target="_blank" rel="noopener noreferrer">Learning and Matching Multi-View Descriptors for Registration of Point Clouds</a>, ECCV 2018</p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Yao_Yao_MVSNet_Depth_Inference_ECCV_2018_paper.pdf" target="_blank" rel="noopener noreferrer">MVSNet: Depth Inference for Unstructured Multi-view Stereo</a>, ECCV 2018</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Poms_Learning_Patch_Reconstructability_CVPR_2018_paper.pdf" target="_blank" rel="noopener noreferrer">Learning Patch Reconstructability for Accelerating Multi-View Stereo</a>, CVPR 2018</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_DeepMVS_Learning_Multi-View_CVPR_2018_paper.pdf" target="_blank" rel="noopener noreferrer">DeepMVS: Learning Multi-View Stereopsis</a>, CVPR 2018</p><p><a href="https://papers.nips.cc/paper/7883-multi-view-silhouette-and-depth-decomposition-for-high-resolution-3d-object-representation.pdf" target="_blank" rel="noopener noreferrer">Multi-View Silhouette and Depth Decomposition for High Resolution 3D Object Representation</a>, NIPS 2018</p><h4 id="_1-3d-reconstruction" tabindex="-1"><a class="header-anchor" href="#_1-3d-reconstruction" aria-hidden="true">#</a> (1) 3D reconstruction</h4><p><a href="https://aaai.org/ojs/index.php/AAAI/article/view/4923/4796" target="_blank" rel="noopener noreferrer">MVPNet: Multi-View Point Regression Networks for 3D Object Reconstruction from A Single Image</a>, AAAI 2019</p><p><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Wei_Conditional_Single-View_Shape_Generation_for_Multi-View_Stereo_Reconstruction_CVPR_2019_paper.pdf" target="_blank" rel="noopener noreferrer">Conditional Single-View Shape Generation for Multi-View Stereo Reconstruction</a>, CVPR 2019</p><p><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Shihao_Wu_Specular-to-Diffuse_Translation_for_ECCV_2018_paper.pdf" target="_blank" rel="noopener noreferrer">Specular-to-Diffuse Translation for Multi-View Reconstruction</a>, ECCV 2018</p><p><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Soltani_Synthesizing_3D_Shapes_CVPR_2017_paper.pdf" target="_blank" rel="noopener noreferrer">Synthesizing 3D Shapes via Modeling Multi-View Depth Maps and Silhouettes With Deep Generative Networks</a>, CVPR 2017</p><p><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Ji_Maximizing_Rigidity_Revisited_ICCV_2017_paper.html" target="_blank" rel="noopener noreferrer">&quot;Maximizing Rigidity&quot; Revisited: A Convex Programming Approach for Generic 3D Shape Reconstruction From Multiple Perspective Views</a>, ICCV 2017</p><p><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Haque_Multi-View_Non-Rigid_Refinement_ICCV_2017_paper.pdf" target="_blank" rel="noopener noreferrer">Multi-View Non-Rigid Refinement and Normal Selection for High Quality 3D Reconstruction</a>, ICCV 2017</p><p><a href="http://openaccess.thecvf.com/content_iccv_2017/html/Leroy_Multi-View_Dynamic_Shape_ICCV_2017_paper.html" target="_blank" rel="noopener noreferrer">Multi-View Dynamic Shape Refinement Using Local Temporal Integration</a>, ICCV 2017</p><p><a href="http://papers.nips.cc/paper/6640-learning-a-multi-view-stereo-machine.pdf" target="_blank" rel="noopener noreferrer">Learning a Multi-View Stereo Machine</a>, NIPS 2017</p><p><a href="https://link.springer.com/article/10.1007/s11263-016-0946-x" target="_blank" rel="noopener noreferrer">A TV Prior for High-Quality Scalable Multi-View Stereo Reconstruction</a>, IJCV 2017名</p></div><!--[--><!--]--></div><footer class="page-meta"><div class="meta-item edit-link"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M497.9 142.1l-46.1 46.1c-4.7 4.7-12.3 4.7-17 0l-111-111c-4.7-4.7-4.7-12.3 0-17l46.1-46.1c18.7-18.7 49.1-18.7 67.9 0l60.1 60.1c18.8 18.7 18.8 49.1 0 67.9zM284.2 99.8L21.6 362.4.4 483.9c-2.9 16.4 11.4 30.6 27.8 27.8l121.5-21.3 262.6-262.6c4.7-4.7 4.7-12.3 0-17l-111-111c-4.8-4.7-12.4-4.7-17.1 0zM124.1 339.9c-5.5-5.5-5.5-14.3 0-19.8l154-154c5.5-5.5 14.3-5.5 19.8 0s5.5 14.3 0 19.8l-154 154c-5.5 5.5-14.3 5.5-19.8 0zM88 424h48v36.3l-64.5 11.3-31.1-31.1L51.7 376H88v48z"/></svg><a class="external-link meta-item-label" href="https://github.com/HenryZhuHR/henryzhuhr.github.io/edit/main/docs/Reseach/myResearch/对抗纹理.md" rel="noopener noreferrer" target="_blank" aria-label="Edit this page on GitHub"><!--[--><!--]--><!----><span>Edit this page on GitHub</span><!--[--><!--]--></a></div><div class="meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><!----></footer><!----><!--[--><!--]--><!----></main><!--]--></div><div class="search-page" role="search"><span class="search-close"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" width="28" height="28" fill="currentColor"><path d="M224 416c-8.188 0-16.38-3.125-22.62-9.375l-192-192c-12.5-12.5-12.5-32.75 0-45.25s32.75-12.5 45.25 0L224 338.8l169.4-169.4c12.5-12.5 32.75-12.5 45.25 0s12.5 32.75 0 45.25l-192 192C240.4 412.9 232.2 416 224 416z"></path></svg></span><div class="gungnir-search-box"><input placeholder="$ grep ..." autocomplete="off" spellcheck="false" value><!----></div></div><div class="menu-btn-container"><div class="menu-btn-wrapper"><div class="menu-btn"><div style="" class="menu-btn-icon"><span></span><span></span><span></span></div><div style="display:none;" class="menu-text">0</div><svg class="menu-progress"><circle class="menu-border" cx="50%" cy="50%" r="48%" style="stroke-dasharray:0% 314.15926%;"></circle></svg></div><div class="menu-btn-child-wrapper"><div title="toggle color mode" class="menu-btn-child"><svg class="ov-icon" style="font-size:1.2em;display:none;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M256 160c-52.9 0-96 43.1-96 96s43.1 96 96 96 96-43.1 96-96-43.1-96-96-96zm246.4 80.5l-94.7-47.3 33.5-100.4c4.5-13.6-8.4-26.5-21.9-21.9l-100.4 33.5-47.4-94.8c-6.4-12.8-24.6-12.8-31 0l-47.3 94.7L92.7 70.8c-13.6-4.5-26.5 8.4-21.9 21.9l33.5 100.4-94.7 47.4c-12.8 6.4-12.8 24.6 0 31l94.7 47.3-33.5 100.5c-4.5 13.6 8.4 26.5 21.9 21.9l100.4-33.5 47.3 94.7c6.4 12.8 24.6 12.8 31 0l47.3-94.7 100.4 33.5c13.6 4.5 26.5-8.4 21.9-21.9l-33.5-100.4 94.7-47.3c13-6.5 13-24.7.2-31.1zm-155.9 106c-49.9 49.9-131.1 49.9-181 0-49.9-49.9-49.9-131.1 0-181 49.9-49.9 131.1-49.9 181 0 49.9 49.9 49.9 131.1 0 181z"/></svg><svg class="ov-icon" style="font-size:1.2em;display:none;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M283.211 512c78.962 0 151.079-35.925 198.857-94.792 7.068-8.708-.639-21.43-11.562-19.35-124.203 23.654-238.262-71.576-238.262-196.954 0-72.222 38.662-138.635 101.498-174.394 9.686-5.512 7.25-20.197-3.756-22.23A258.156 258.156 0 00283.211 0c-141.309 0-256 114.511-256 256 0 141.309 114.511 256 256 256z"/></svg><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-43.52 -43.52 599.04 599.04" fill="currentColor"><path d="M224 96l16-32 32-16-32-16-16-32-16 32-32 16 32 16 16 32zM80 160l26.66-53.33L160 80l-53.34-26.67L80 0 53.34 53.33 0 80l53.34 26.67L80 160zm352 128l-26.66 53.33L352 368l53.34 26.67L432 448l26.66-53.33L512 368l-53.34-26.67L432 288zm70.62-193.77L417.77 9.38C411.53 3.12 403.34 0 395.15 0c-8.19 0-16.38 3.12-22.63 9.38L9.38 372.52c-12.5 12.5-12.5 32.76 0 45.25l84.85 84.85c6.25 6.25 14.44 9.37 22.62 9.37 8.19 0 16.38-3.12 22.63-9.37l363.14-363.15c12.5-12.48 12.5-32.75 0-45.24zM359.45 203.46l-50.91-50.91 86.6-86.6 50.91 50.91-86.6 86.6z"/></svg></div><div class="menu-btn-child"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"/></svg></div><div class="menu-btn-child"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-75.52 -43.52 599.04 599.04" fill="currentColor"><path d="M240.971 130.524l194.343 194.343c9.373 9.373 9.373 24.569 0 33.941l-22.667 22.667c-9.357 9.357-24.522 9.375-33.901.04L224 227.495 69.255 381.516c-9.379 9.335-24.544 9.317-33.901-.04l-22.667-22.667c-9.373-9.373-9.373-24.569 0-33.941L207.03 130.525c9.372-9.373 24.568-9.373 33.941-.001z"/></svg></div><!----><div class="toggle-sidebar-button menu-btn-child menu-btn-sidebar" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><svg class="ov-icon" style="font-size:1.2em;" aria-hidden="true" width="19.2" height="19.2" viewBox="-1.6 -1.6 19.2 19.2" fill="currentColor"><path d="M14 2a1 1 0 011 1v10a1 1 0 01-1 1H2a1 1 0 01-1-1V3a1 1 0 011-1h12zM2 1a2 2 0 00-2 2v10a2 2 0 002 2h12a2 2 0 002-2V3a2 2 0 00-2-2H2z"/><path d="M3 4a1 1 0 011-1h2a1 1 0 011 1v8a1 1 0 01-1 1H4a1 1 0 01-1-1V4z"/></svg></div></div></div></div><!----></div><!--]--></div>
    <script type="module" src="/assets/app.e2a6bb87.js" defer></script>
  </body>
</html>
